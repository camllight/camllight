\chapter{Lexer and parser generators (camllex, camlyacc)}
\label{c:camlyacc}

This chapter describes two program generators: "camllex", that
produces a lexical analyzer from a set of regular expressions with
associated semantic actions, and "camlyacc", that produces a parser
from a grammar with associated semantic actions.

These program generators are very close to the well-known "lex" and
"yacc" commands that can be found in most C programming environments.
This chapter assumes a working knowledge of "lex" and "yacc": while
it describes the input syntax for "camllex" and "camlyacc" and the
main differences with "lex" and "yacc", it does not explain the basics
of writing a lexer or parser description in "lex" and "yacc". Readers
unfamiliar with "lex" and "yacc" are referred to  ``Compilers:
principles, techniques, and tools'' by Aho, Sethi and Ullman
(Addison-Wesley, 1986), ``Compiler design in~C'' by Holub
(Prentice-Hall, 1990), or ``Lex $\&$ Yacc'', by Mason and Brown
(O'Reilly, 1990).

Streams and stream matching, as described in section~\ref{s:streams},
provide an alternative way to write lexers and parsers. The stream
matching technique is more powerful than the combination of "camllex"
and "camlyacc" in some cases (higher-order parsers), but less powerful
in other cases (precedences). Choose whichever approach is
more adapted to your parsing problem.

\begin{mac} These commands are MPW tool, not standalone Macintosh
applications.
\end{mac}

\section{Overview of "camllex"}


The "camllex" command produces a lexical analyzer from a set of regular
expressions with attached semantic actions, in the style of
"lex". Assuming the input file is ${\it lexer}".mll"$, executing
\begin{alltt}
        camllex \var{lexer}.mll
\end{alltt}
produces Caml Light code for a lexical analyzer in file \var{lexer}".ml".
This file defines one lexing function per entry point in the lexer
definition. These functions have the same names as the entry
points. Lexing functions take as argument a lexer buffer, and return
the semantic attribute of the corresponding entry point.

Lexer buffers are an abstract data type implemented in the standard
library module "lexing". The functions "create_lexer_channel",
"create_lexer_string" and "create_lexer" from module "lexing" create
lexer buffers that read from an input channel, a character string, or
any reading function, respectively. (See the description of module
"lexing" in chapter~\ref{c:stdlib}.)

When used in conjunction with a parser generated by "camlyacc", the
semantic actions compute a value belonging to the type "token" defined
by the generated parsing module. (See the description of "camlyacc"
below.)

\section{Syntax of lexer definitions}

The format of lexer definitions is as follows:
\begin{alltt}
\{ \var{header} \}
rule \var{entrypoint} =
  parse \var{regexp} \{ \var{action} \}
      | \ldots
      | \var{regexp} \{ \var{action} \}
and \var{entrypoint} =
  parse \ldots
and \ldots
;;
\end{alltt}

Comments are delimited by \verb|(*| and \verb|*)|, as in Caml Light.

\subsection{Header}
The {\it header} section is arbitrary Caml Light text enclosed in
curly braces. It can be omitted. If it is present, the enclosed text
is copied as is at the beginning of the output file. Typically, the
header section contains the \verb"#open" directives required
by the actions, and possibly some auxiliary functions used in the actions.

\subsection{Entry points}

The names of the entry points must be valid Caml Light identifiers.

\subsection{Regular expressions}

The regular expressions are in the style of "lex", with a more
Caml-like syntax.

\begin{options}

\item[@"`" char "`"@]
A character constant, with the same syntax as Caml Light character
constants. Match the denoted character.

\item["_"]
Match any character.

\item["eof"]
Match the end of the lexer input.

\item[@'"' string '"'@]
A string constant, with the same syntax as Caml Light string
constants. Match the corresponding sequence of characters.

\item[@'[' character-set ']'@]
Match any single character belonging to the given
character set. Valid character sets are: single
character constants @"`" c "`"@; ranges of characters
@"`" c_1 "`" "-" "`" c_2 "`"@ (all characters between $c_1$ and $c_2$,
inclusive); and the union of two or more character sets, denoted by
concatenation.

\item[@'[' '^' character-set ']'@]
Match any single character not belonging to the given character set.


\item[@regexp '*'@]
(Repetition.) Match the concatenation of zero or more
strings that match @regexp@. 

\item[@regexp '+'@]
(Strict repetition.) Match the concatenation of one or more
strings that match @regexp@.

\item[@regexp '?'@]
(Option.) Match either the empty string, or a string matching @regexp@.

\item[@regexp_1 '|' regexp_2@]
(Alternative.) Match any string that matches either @regexp_1@ or @regexp_2@

\item[@regexp_1 regexp_2@]
(Concatenation.) Match the concatenation of two strings, the first
matching @regexp_1@, the second matching @regexp_2@.

\item[@'(' regexp ')'@]
Match the same strings as @regexp@.

\end{options}

Concerning the precedences of operators, \verb"*" and \verb"+" have
highest precedence, followed by \verb'?', then concatenation, then
\verb"|" (alternation).

\subsection{Actions}

The actions are arbitrary Caml Light expressions. They are evaluated in
a context where the identifier "lexbuf" is bound to the current lexer
buffer. Some typical uses for "lexbuf", in conjunction with the
operations on lexer buffers provided by the "lexing" standard library
module, are listed below.

\begin{options}
\item["lexing__get_lexeme lexbuf"]
Return the matched string.

\item["lexing__get_lexeme_char lexbuf "$n$]
Return the
\begin{latexonly}$n^{\mbox{\scriptsize th}}$\end{latexonly}
\begin{rawhtml}
<I>n</I>-th
\end{rawhtml}
character in the matched string. The first character corresponds to $n = 0$.

\item["lexing__get_lexeme_start lexbuf"]
Return the absolute position in the input text of the beginning of the
matched string. The first character read from the input text has
position 0.

\item["lexing__get_lexeme_end lexbuf"]
Return the absolute position in the input text of the end of the
matched string. The first character read from the input text has
position 0.

\item[\var{entrypoint} "lexbuf"]
(Where \var{entrypoint} is the name of another entry point in the same
lexer definition.) Recursively call the lexer on the given entry point.
Useful for lexing nested comments, for example.

\end{options}

\section{Overview of "camlyacc"}

The "camlyacc" command produces a parser from a context-free grammar
specification with attached semantic actions, in the style of "yacc".
Assuming the input file is $grammar".mly"$, executing
\begin{alltt}
        camlyacc \var{options} \var{grammar}.mly
\end{alltt}
produces Caml Light code for a parser in the file \var{grammar}".ml",
and its interface in file \var{grammar}".mli".

The generated module defines one parsing function per entry point in
the grammar. These functions have the same names as the entry points.
Parsing functions take as arguments a lexical analyzer (a function
from lexer buffers to tokens) and a lexer buffer, and return the
semantic attribute of the corresponding entry point. Lexical analyzer
functions are usually generated from a lexer specification by the
"camllex" program. Lexer buffers are an abstract data type
implemented in the standard library module "lexing". Tokens are values from
the concrete type "token", defined in the interface file
\var{grammar}".mli" produced by "camlyacc".

\section{Syntax of grammar definitions}

Grammar definitions have the following format:
\begin{alltt}
%\{
  \var{header}
%\}
  \var{declarations}
%%
  \var{rules}
%%
  \var{trailer}
\end{alltt}

Comments are enclosed between \verb|/*| and \verb|*/| (as in C) in the
``declarations'' and ``rules'' sections, and between \verb|(*| and
\verb|*)| (as in Caml) in the ``header'' and ``trailer'' sections.

\subsection{Header and trailer}

The header and the trailer sections are Caml Light code that is copied
as is into file \var{grammar}".ml". Both sections are optional. The header
goes at the beginning of the output file; it usually contains
\verb"#open" directives required by the semantic actions of the rules.
The trailer goes at the end of the output file.

\subsection{Declarations}

Declarations are given one per line. They all start with a \verb"%" sign.

\begin{options}

\item[@"%token" symbol \ldots symbol@]
Declare the given symbols as tokens (terminal symbols).  These symbols
are added as constant constructors for the "token" concrete type.

\item[@"%token" "<" type ">" symbol \ldots symbol@]
Declare the given symbols as tokens with an attached attribute of the
given type. These symbols are added as constructors with arguments of
the given type for the "token" concrete type. The @type@ part is
an arbitrary Caml Light type expression, except that all type
constructor names must be fully qualified (e.g. "modname__typename")
for all types except standard built-in types, even if the proper
\verb|#open| directives (e.g. \verb|#open "modname"|) were given in the
header section. That's because the header is copied only to the ".ml"
output file, but not to the ".mli" output file, while the @type@ part
of a \verb"%token" declaration is copied to both.

\item[@"%start" symbol \ldots symbol@]
Declare the given symbols as entry points for the grammar. For each
entry point, a parsing function with the same name is defined in the
output module. Non-terminals that are not declared as entry points
have no such parsing function. Start symbols must be given a type with
the \verb|%type| directive below.

\item[@"%type" "<" type ">" symbol \ldots symbol@]
Specify the type of the semantic attributes for the given symbols.
This is mandatory for start symbols only. Other nonterminal symbols
need not be given types by hand: these types will be inferred when
running the output files through the Caml Light compiler (unless the
\verb"-s" option is in effect). The @type@ part is an arbitrary Caml
Light type expression, except that all type constructor names must be
fully qualified (e.g. "modname__typename") for all types except
standard built-in types, even if the proper \verb|#open| directives (e.g.
\verb|#open "modname"|) were given in the header section. That's
because the header is copied only to the ".ml" output file, but not to
the ".mli" output file, while the @type@ part of a \verb"%token"
declaration is copied to both.

\item[@"%left" symbol \ldots symbol@]
\item[@"%right" symbol \ldots symbol@]
\item[@"%nonassoc" symbol \ldots symbol@]

Associate precedences and associativities to the given symbols. All
symbols on the same line are given the same precedence. They have
higher precedence than symbols declared before in a \verb"%left",
\verb"%right" or \verb"%nonassoc" line. They have lower precedence
than symbols declared after in a \verb"%left", \verb"%right" or
\verb"%nonassoc" line. The symbols are declared to associate to the
left (\verb"%left"), to the right (\verb"%right"), or to be
non-associative (\verb"%nonassoc"). The symbols are usually tokens.
They can also be dummy nonterminals, for use with the \verb"%prec"
directive inside the rules.

\end{options}

\subsection{Rules}

The syntax for rules is as usual:
\begin{alltt}
\var{nonterminal} :
    \var{symbol} \ldots \var{symbol} \{ \var{semantic-action} \}
  | \ldots
  | \var{symbol} \ldots \var{symbol} \{ \var{semantic-action} \}
;
\end{alltt}
%
Rules can also contain the \verb"%prec "{\it symbol} directive in the
right-hand side part, to override the default precedence and
associativity of the rule with the precedence and associativity of the
given symbol.

Semantic actions are arbitrary Caml Light expressions, that
are evaluated to produce the semantic attribute attached to
the defined nonterminal. The semantic actions can access the
semantic attributes of the symbols in the right-hand side of
the rule with the \verb"$" notation: \verb"$1" is the attribute for the
first (leftmost) symbol, \verb"$2" is the attribute for the second
symbol, etc.

Actions occurring in the middle of rules are not supported. Error
recovery is not implemented.

\section{Options}

The "camlyacc" command recognizes the following options:

\begin{options}
\item["-v"]
Generate a description of the parsing tables and a report on conflicts
resulting from ambiguities in the grammar. The description is put in
file \var{grammar}".output".

\item["-s"]
Generate a \var{grammar}".ml" file with smaller phrases.  Semantic actions
are presented in the \var{grammar}".ml" output file as one large vector of
functions. By default, this vector is built by a single phrase. When
the grammar is large, or contains complicated semantic actions, the
resulting phrase may require large amounts of memory to be compiled by
Caml Light. With the "-s" option, the vector of actions is constructed
incrementally, one phrase per action. This lowers the memory
requirements for the compiler, but it is no longer possible to infer
the types of nonterminal symbols: typechecking is turned off on
symbols that do not have a type specified by a \verb"%type" directive.

\item["-b"{\it prefix}]
Name the output files {\it prefix}".ml", {\it prefix}".mli",
{\it prefix}".output", instead of the default naming convention.

\end{options}

\section{A complete example}

The all-time favorite: a desk calculator. This program reads
arithmetic expressions on standard input, one per line, and prints
their values. Here is the grammar definition:
\begin{verbatim}
        /* File parser.mly */
        %token <int> INT
        %token PLUS MINUS TIMES DIV
        %token LPAREN RPAREN
        %token EOL
        %left PLUS MINUS        /* lowest precedence */
        %left TIMES DIV         /* medium precedence */
        %nonassoc UMINUS        /* highest precedence */
        %start Main             /* the entry point */
        %type <int> Main
        %%
        Main:
            Expr EOL                { $1 }
        ;
        Expr:
            INT                     { $1 }
          | LPAREN Expr RPAREN      { $2 }
          | Expr PLUS Expr          { $1 + $3 }
          | Expr MINUS Expr         { $1 - $3 }
          | Expr TIMES Expr         { $1 * $3 }
          | Expr DIV Expr           { $1 / $3 }
          | MINUS Expr %prec UMINUS { - $2 }
        ;
\end{verbatim}
Here is the definition for the corresponding lexer:
\begin{verbatim}
        (* File lexer.mll *)
        {
        #open "parser";;        (* The type token is defined in parser.mli *)
        exception Eof;;
        }
        rule Token = parse
            [` ` `\t`]     { Token lexbuf }     (* skip blanks *)
          | [`\n` ]        { EOL }
          | [`0`-`9`]+     { INT(int_of_string (get_lexeme lexbuf)) }
          | `+`            { PLUS }
          | `-`            { MINUS }
          | `*`            { TIMES }
          | `/`            { DIV }
          | `(`            { LPAREN }
          | `)`            { RPAREN }
          | eof            { raise Eof }
        ;;
\end{verbatim}
Here is the main program, that combines the parser with the lexer:
\begin{verbatim}
        (* File calc.ml *)
        try
          let lexbuf = lexing__create_lexer_channel std_in in
          while true do
            let result = parser__Main lexer__Token lexbuf in
              print_int result; print_newline(); flush std_out
          done
        with Eof ->
          ()
        ;;
\end{verbatim}
To compile everything, execute:
\begin{verbatim}
        camllex lexer.mll       # generates lexer.ml
        camlyacc parser.mly     # generates parser.ml and parser.mli
        camlc -c parser.mli
        camlc -c lexer.ml
        camlc -c parser.ml
        camlc -c calc.ml
        camlc -o calc lexer.zo parser.zo calc.zo
\end{verbatim}
